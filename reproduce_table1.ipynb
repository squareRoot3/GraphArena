{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tasks import *\n",
    "import numpy as np\n",
    "\n",
    "path = 'final_results/np_easy'  # 'np_hard' or 'p_easy' or 'p_hard' \n",
    "difficulty = 'easy'\n",
    "files = os.listdir(path)\n",
    "merged_responses = {}\n",
    "problem_num = 500\n",
    "dataset_loc = 'dataset'\n",
    "\n",
    "for f in files:\n",
    "    if len(f.split('_')) < 2:\n",
    "        continue\n",
    "    llm, task = f.split('_')[0], f.split('_')[1]\n",
    "    with open(f'{path}/{f}', 'r') as file:\n",
    "        response_dict = json.load(file)\n",
    "    for i in range(0, problem_num):\n",
    "        if task not in merged_responses:\n",
    "            merged_responses[task] = defaultdict(dict)\n",
    "        merged_responses[task][i][llm] = response_dict[str(i)][llm]\n",
    "task_list = list(merged_responses.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invalid literal for int() with base 10: ''\n",
      "invalid literal for int() with base 10: ''\n",
      "invalid literal for int() with base 10: ''\n"
     ]
    }
   ],
   "source": [
    "# The scoring process may take a few minutes.\n",
    "# If want to evaluate on specific tasks and models:\n",
    "# task_list = ['GED', 'MCP', 'MCS', 'TSP']\n",
    "# model_list = ['deepseek', 'llama', 'llama8b', 'gpt', 'gpt4', 'claude', 'mixtral']\n",
    "score = {}\n",
    "for task_name in task_list:\n",
    "    task= globals()[task_name + '_Task'](dataset_loc)\n",
    "    task.load_dataset(difficulty)\n",
    "    score[task_name] = defaultdict(dict)\n",
    "    for i in range(0, problem_num):\n",
    "        score[task_name][i]['gt'] = task.problem_set[i]['exact_answer']\n",
    "        if task_name in ['GED', 'TSP', 'MCS'] and difficulty=='hard':\n",
    "            score[task_name][i]['gt'] = task.problem_set[i]['approx_answer']\n",
    "        for llm in merged_responses[task_name][i].keys():\n",
    "            if llm == 'problem':\n",
    "                continue\n",
    "            r = merged_responses[task_name][i][llm]\n",
    "            if r is None:\n",
    "                r = ''\n",
    "                print(i, llm, task_name)\n",
    "            score[task_name][i][llm] = task.check_solution(i, r)\n",
    "# json.dump(score, open('score.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP\n",
      "MCS\n",
      "MVC\n",
      "MIS\n",
      "GED\n",
      "TSP\n"
     ]
    }
   ],
   "source": [
    "metrics = defaultdict(dict)\n",
    "less_is_better = ['GED', 'TSP', 'MVC', 'Distance']\n",
    "results = []\n",
    "for task in task_list:\n",
    "    print(task)\n",
    "    model_list = list(score[task][0].keys())\n",
    "    model_list.remove('gt')\n",
    "    for model in model_list:\n",
    "        metrics[task][model] = {'rank':[], 'feasible':[], 'MRR':[], 'hallu':[], 'acc': [],'top1':[], 'top3':[], 'len': []}\n",
    "        for i in range(0, problem_num):\n",
    "            metrics[task][model]['feasible'].append(score[task][i][model]>0)\n",
    "            metrics[task][model]['hallu'].append(score[task][i][model]==-2)\n",
    "            metrics[task][model]['len'].append(len(merged_responses[task_name][i][model]))\n",
    "            if task in ['GED', 'TSP', 'MVC']:\n",
    "                acc = 0 <= score[task][i][model] and score[task][i][model] <= score[task][i]['gt']\n",
    "            elif task in ['MCP', 'MCS', 'MIC']:\n",
    "                acc = score[task][i][model] >= score[task][i]['gt']\n",
    "            else:\n",
    "                acc = score[task][i][model] == score[task][i]['gt']\n",
    "            \n",
    "            metrics[task][model]['acc'].append(acc)\n",
    "            \n",
    "            rank = 1\n",
    "            error_knt = 0\n",
    "            for model2 in model_list:\n",
    "                if score[task][i][model2] < 0:\n",
    "                    error_knt += 1\n",
    "                if task in less_is_better:\n",
    "                    if score[task][i][model] > score[task][i][model2] and score[task][i][model2] >= 0:\n",
    "                        rank += 1\n",
    "                else:\n",
    "                    if score[task][i][model] < score[task][i][model2] and score[task][i][model2] >= 0:\n",
    "                        rank += 1\n",
    "            if score[task][i][model] < 0:\n",
    "                rank = len(model_list)\n",
    "            if error_knt == len(model_list):\n",
    "                continue\n",
    "            metrics[task][model]['rank'].append(rank)\n",
    "            metrics[task][model]['top1'].append(rank==1)\n",
    "            metrics[task][model]['top3'].append(rank<=3)\n",
    "            metrics[task][model]['MRR'].append(1/rank)\n",
    "        avg_rank = np.mean(metrics[task][model]['rank'])\n",
    "        avg_feasible = sum(metrics[task][model]['feasible']) / problem_num\n",
    "        avg_MRR = np.mean(metrics[task][model]['MRR'])\n",
    "        avg_hallu = sum(metrics[task][model]['hallu']) / problem_num\n",
    "        avg_acc = sum(metrics[task][model]['acc']) / problem_num\n",
    "        avg_top1 = np.mean(metrics[task][model]['top1'])\n",
    "        avg_top3 = np.mean(metrics[task][model]['top3'])\n",
    "        avg_len = np.mean(metrics[task][model]['len'])\n",
    "        results.append((task, model, avg_top1, avg_top3, avg_MRR, avg_feasible, avg_hallu, avg_acc, avg_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: MCP\n",
      "Model: gpt4, Top1: 0.594, Top3: 0.654, MRR: 0.661, Feasible: 0.692, Hallucination: 0.306, Accuracy: 0.528, len: 1084.416\n",
      "Model: claude, Top1: 0.550, Top3: 0.605, MRR: 0.620, Feasible: 0.646, Hallucination: 0.348, Accuracy: 0.482, len: 38.608\n",
      "Model: qwen, Top1: 0.448, Top3: 0.514, MRR: 0.536, Feasible: 0.610, Hallucination: 0.374, Accuracy: 0.388, len: 81.378\n",
      "Model: gpt, Top1: 0.444, Top3: 0.484, MRR: 0.517, Feasible: 0.526, Hallucination: 0.474, Accuracy: 0.400, len: 33.802\n",
      "Model: llama, Top1: 0.456, Top3: 0.465, MRR: 0.514, Feasible: 0.442, Hallucination: 0.558, Accuracy: 0.428, len: 32.544\n",
      "Model: qwen7b, Top1: 0.248, Top3: 0.291, MRR: 0.347, Feasible: 0.442, Hallucination: 0.548, Accuracy: 0.216, len: 65.506\n",
      "Model: gemma, Top1: 0.276, Top3: 0.310, MRR: 0.368, Feasible: 0.436, Hallucination: 0.562, Accuracy: 0.254, len: 30.720\n",
      "Model: deepseek, Top1: 0.433, Top3: 0.448, MRR: 0.496, Feasible: 0.428, Hallucination: 0.570, Accuracy: 0.400, len: 894.602\n",
      "Model: llama8b, Top1: 0.310, Top3: 0.333, MRR: 0.394, Feasible: 0.428, Hallucination: 0.566, Accuracy: 0.280, len: 199.992\n",
      "Model: mixtral, Top1: 0.304, Top3: 0.329, MRR: 0.387, Feasible: 0.384, Hallucination: 0.616, Accuracy: 0.278, len: 207.976\n",
      "\n",
      "Task: MCS\n",
      "Model: deepseek, Top1: 0.350, Top3: 0.565, MRR: 0.510, Feasible: 0.804, Hallucination: 0.190, Accuracy: 0.236, len: 894.602\n",
      "Model: llama8b, Top1: 0.179, Top3: 0.356, MRR: 0.351, Feasible: 0.758, Hallucination: 0.242, Accuracy: 0.096, len: 199.992\n",
      "Model: qwen, Top1: 0.481, Top3: 0.685, MRR: 0.608, Feasible: 0.754, Hallucination: 0.246, Accuracy: 0.298, len: 81.378\n",
      "Model: llama, Top1: 0.654, Top3: 0.746, MRR: 0.723, Feasible: 0.724, Hallucination: 0.276, Accuracy: 0.442, len: 32.544\n",
      "Model: claude, Top1: 0.431, Top3: 0.646, MRR: 0.565, Feasible: 0.702, Hallucination: 0.298, Accuracy: 0.282, len: 38.608\n",
      "Model: gpt, Top1: 0.315, Top3: 0.506, MRR: 0.465, Feasible: 0.700, Hallucination: 0.300, Accuracy: 0.176, len: 33.802\n",
      "Model: gpt4, Top1: 0.588, Top3: 0.642, MRR: 0.652, Feasible: 0.656, Hallucination: 0.296, Accuracy: 0.406, len: 1084.416\n",
      "Model: mixtral, Top1: 0.146, Top3: 0.237, MRR: 0.276, Feasible: 0.412, Hallucination: 0.574, Accuracy: 0.098, len: 207.976\n",
      "Model: gemma, Top1: 0.090, Top3: 0.196, MRR: 0.232, Feasible: 0.376, Hallucination: 0.624, Accuracy: 0.020, len: 30.720\n",
      "Model: qwen7b, Top1: 0.133, Top3: 0.215, MRR: 0.256, Feasible: 0.304, Hallucination: 0.690, Accuracy: 0.062, len: 65.506\n",
      "\n",
      "Task: MVC\n",
      "Model: gemma, Top1: 0.164, Top3: 0.247, MRR: 0.305, Feasible: 0.584, Hallucination: 0.416, Accuracy: 0.128, len: 30.720\n",
      "Model: deepseek, Top1: 0.458, Top3: 0.538, MRR: 0.548, Feasible: 0.582, Hallucination: 0.418, Accuracy: 0.376, len: 894.602\n",
      "Model: claude, Top1: 0.427, Top3: 0.500, MRR: 0.521, Feasible: 0.580, Hallucination: 0.418, Accuracy: 0.336, len: 38.608\n",
      "Model: llama8b, Top1: 0.362, Top3: 0.429, MRR: 0.460, Feasible: 0.500, Hallucination: 0.500, Accuracy: 0.318, len: 199.992\n",
      "Model: llama, Top1: 0.478, Top3: 0.509, MRR: 0.543, Feasible: 0.474, Hallucination: 0.526, Accuracy: 0.420, len: 32.544\n",
      "Model: gpt4, Top1: 0.393, Top3: 0.440, MRR: 0.476, Feasible: 0.458, Hallucination: 0.542, Accuracy: 0.326, len: 1084.416\n",
      "Model: gpt, Top1: 0.438, Top3: 0.471, MRR: 0.508, Feasible: 0.452, Hallucination: 0.548, Accuracy: 0.376, len: 33.802\n",
      "Model: mixtral, Top1: 0.158, Top3: 0.216, MRR: 0.280, Feasible: 0.438, Hallucination: 0.562, Accuracy: 0.130, len: 207.976\n",
      "Model: qwen7b, Top1: 0.160, Top3: 0.200, MRR: 0.274, Feasible: 0.364, Hallucination: 0.636, Accuracy: 0.138, len: 65.506\n",
      "Model: qwen, Top1: 0.258, Top3: 0.269, MRR: 0.337, Feasible: 0.260, Hallucination: 0.738, Accuracy: 0.224, len: 81.378\n",
      "\n",
      "Task: MIS\n",
      "Model: llama, Top1: 0.454, Top3: 0.629, MRR: 0.589, Feasible: 0.910, Hallucination: 0.080, Accuracy: 0.368, len: 32.544\n",
      "Model: claude, Top1: 0.522, Top3: 0.675, MRR: 0.633, Feasible: 0.800, Hallucination: 0.198, Accuracy: 0.450, len: 38.608\n",
      "Model: gpt4, Top1: 0.604, Top3: 0.695, MRR: 0.681, Feasible: 0.792, Hallucination: 0.200, Accuracy: 0.518, len: 1084.416\n",
      "Model: qwen, Top1: 0.442, Top3: 0.580, MRR: 0.561, Feasible: 0.792, Hallucination: 0.204, Accuracy: 0.386, len: 81.378\n",
      "Model: deepseek, Top1: 0.420, Top3: 0.560, MRR: 0.543, Feasible: 0.778, Hallucination: 0.222, Accuracy: 0.360, len: 894.602\n",
      "Model: gpt, Top1: 0.213, Top3: 0.337, MRR: 0.360, Feasible: 0.764, Hallucination: 0.236, Accuracy: 0.184, len: 33.802\n",
      "Model: mixtral, Top1: 0.157, Top3: 0.263, MRR: 0.308, Feasible: 0.756, Hallucination: 0.240, Accuracy: 0.116, len: 207.976\n",
      "Model: llama8b, Top1: 0.345, Top3: 0.432, MRR: 0.455, Feasible: 0.616, Hallucination: 0.378, Accuracy: 0.282, len: 199.992\n",
      "Model: qwen7b, Top1: 0.133, Top3: 0.201, MRR: 0.262, Feasible: 0.520, Hallucination: 0.480, Accuracy: 0.118, len: 65.506\n",
      "Model: gemma, Top1: 0.225, Top3: 0.263, MRR: 0.322, Feasible: 0.344, Hallucination: 0.656, Accuracy: 0.212, len: 30.720\n",
      "\n",
      "Task: GED\n",
      "Model: claude, Top1: 0.390, Top3: 0.596, MRR: 0.552, Feasible: 0.992, Hallucination: 0.004, Accuracy: 0.216, len: 38.608\n",
      "Model: gpt, Top1: 0.296, Top3: 0.558, MRR: 0.489, Feasible: 0.978, Hallucination: 0.010, Accuracy: 0.144, len: 33.802\n",
      "Model: gemma, Top1: 0.118, Top3: 0.392, MRR: 0.342, Feasible: 0.974, Hallucination: 0.012, Accuracy: 0.028, len: 30.720\n",
      "Model: llama, Top1: 0.482, Top3: 0.712, MRR: 0.631, Feasible: 0.968, Hallucination: 0.030, Accuracy: 0.316, len: 32.544\n",
      "Model: gpt4, Top1: 0.414, Top3: 0.592, MRR: 0.558, Feasible: 0.898, Hallucination: 0.044, Accuracy: 0.268, len: 1084.416\n",
      "Model: deepseek, Top1: 0.436, Top3: 0.656, MRR: 0.585, Feasible: 0.862, Hallucination: 0.064, Accuracy: 0.282, len: 894.602\n",
      "Model: qwen, Top1: 0.282, Top3: 0.432, MRR: 0.433, Feasible: 0.826, Hallucination: 0.010, Accuracy: 0.174, len: 81.378\n",
      "Model: mixtral, Top1: 0.188, Top3: 0.306, MRR: 0.331, Feasible: 0.648, Hallucination: 0.190, Accuracy: 0.124, len: 207.976\n",
      "Model: qwen7b, Top1: 0.104, Top3: 0.204, MRR: 0.251, Feasible: 0.592, Hallucination: 0.408, Accuracy: 0.058, len: 65.506\n",
      "Model: llama8b, Top1: 0.120, Top3: 0.160, MRR: 0.226, Feasible: 0.226, Hallucination: 0.762, Accuracy: 0.072, len: 199.992\n",
      "\n",
      "Task: TSP\n",
      "Model: llama, Top1: 0.326, Top3: 0.568, MRR: 0.508, Feasible: 0.998, Hallucination: 0.002, Accuracy: 0.232, len: 32.544\n",
      "Model: deepseek, Top1: 0.512, Top3: 0.754, MRR: 0.666, Feasible: 0.994, Hallucination: 0.004, Accuracy: 0.368, len: 894.602\n",
      "Model: gpt, Top1: 0.292, Top3: 0.458, MRR: 0.454, Feasible: 0.970, Hallucination: 0.012, Accuracy: 0.230, len: 33.802\n",
      "Model: claude, Top1: 0.290, Top3: 0.464, MRR: 0.456, Feasible: 0.958, Hallucination: 0.000, Accuracy: 0.242, len: 38.608\n",
      "Model: llama8b, Top1: 0.192, Top3: 0.294, MRR: 0.345, Feasible: 0.944, Hallucination: 0.056, Accuracy: 0.162, len: 199.992\n",
      "Model: gpt4, Top1: 0.548, Top3: 0.748, MRR: 0.677, Feasible: 0.944, Hallucination: 0.006, Accuracy: 0.404, len: 1084.416\n",
      "Model: qwen, Top1: 0.264, Top3: 0.408, MRR: 0.419, Feasible: 0.924, Hallucination: 0.072, Accuracy: 0.228, len: 81.378\n",
      "Model: mixtral, Top1: 0.234, Top3: 0.338, MRR: 0.379, Feasible: 0.922, Hallucination: 0.064, Accuracy: 0.188, len: 207.976\n",
      "Model: qwen7b, Top1: 0.182, Top3: 0.250, MRR: 0.308, Feasible: 0.780, Hallucination: 0.216, Accuracy: 0.170, len: 65.506\n",
      "Model: gemma, Top1: 0.144, Top3: 0.194, MRR: 0.269, Feasible: 0.714, Hallucination: 0.286, Accuracy: 0.134, len: 30.720\n",
      "\n",
      "Average Performance Across All Tasks:\n",
      "Model: gpt4, Average MRR: 0.617, Average Top1: 0.524, Feasible: 0.740, Hallucination: 0.232, Accuracy: 0.408, len: 1084.416\n",
      "Model: llama, Average MRR: 0.585, Average Top1: 0.475, Feasible: 0.753, Hallucination: 0.245, Accuracy: 0.368, len: 32.544\n",
      "Model: deepseek, Average MRR: 0.558, Average Top1: 0.435, Feasible: 0.741, Hallucination: 0.245, Accuracy: 0.337, len: 894.602\n",
      "Model: claude, Average MRR: 0.558, Average Top1: 0.435, Feasible: 0.780, Hallucination: 0.211, Accuracy: 0.335, len: 38.608\n",
      "Model: qwen, Average MRR: 0.482, Average Top1: 0.362, Feasible: 0.694, Hallucination: 0.274, Accuracy: 0.283, len: 81.378\n",
      "Model: gpt, Average MRR: 0.465, Average Top1: 0.333, Feasible: 0.732, Hallucination: 0.263, Accuracy: 0.252, len: 33.802\n",
      "Model: llama8b, Average MRR: 0.372, Average Top1: 0.251, Feasible: 0.579, Hallucination: 0.417, Accuracy: 0.202, len: 199.992\n",
      "Model: mixtral, Average MRR: 0.327, Average Top1: 0.198, Feasible: 0.593, Hallucination: 0.374, Accuracy: 0.156, len: 207.976\n",
      "Model: gemma, Average MRR: 0.306, Average Top1: 0.169, Feasible: 0.571, Hallucination: 0.426, Accuracy: 0.129, len: 30.720\n",
      "Model: qwen7b, Average MRR: 0.283, Average Top1: 0.160, Feasible: 0.500, Hallucination: 0.496, Accuracy: 0.127, len: 65.506\n"
     ]
    }
   ],
   "source": [
    "# Sorting the results by MRR for each task\n",
    "sorted_results = defaultdict(list)\n",
    "\n",
    "for task in task_list:\n",
    "    task_results = [result for result in results if result[0] == task]\n",
    "    sorted_results[task] = sorted(task_results, key=lambda x: x[5], reverse=True)  # Sort by MRR\n",
    "\n",
    "# Print sorted results for each task\n",
    "for task, task_results in sorted_results.items():\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    for result in task_results:\n",
    "        print(f\"Model: {result[1]}, Top1: {result[2]:.3f}, Top3: {result[3]:.3f}, MRR: {result[4]:.3f}, Feasible: {result[5]:.3f}, Hallucination: {result[6]:.3f}, Accuracy: {result[7]:.3f}, len: {result[8]:.3f}\")\n",
    "\n",
    "# Calculate average MRR performance across all tasks for each model\n",
    "model_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Aggregate metrics for each model across tasks\n",
    "for result in results:\n",
    "    task, model, avg_top1, avg_top3, avg_MRR, avg_feasible, avg_hallu, avg_acc, avg_len = result\n",
    "    model_metrics[model]['MRR'].append(avg_MRR)\n",
    "    model_metrics[model]['top1'].append(avg_top1)\n",
    "    model_metrics[model]['top3'].append(avg_top3)\n",
    "    model_metrics[model]['feasible'].append(avg_feasible)\n",
    "    model_metrics[model]['hallu'].append(avg_hallu)\n",
    "    model_metrics[model]['acc'].append(avg_acc)\n",
    "    model_metrics[model]['len'].append(avg_len)\n",
    "    \n",
    "# Compute average metrics for each model and sort models by their average MRR\n",
    "average_metrics_performance = {model: {metric: sum(values) / len(values) for metric, values in metrics.items()} for model, metrics in model_metrics.items()}\n",
    "sorted_average_metrics = sorted(average_metrics_performance.items(), key=lambda x: x[1]['MRR'], reverse=True)\n",
    "\n",
    "# Print the sorted average metrics for each model\n",
    "print(\"\\nAverage Performance Across All Tasks:\")\n",
    "for model, metrics in sorted_average_metrics:\n",
    "    print(f\"Model: {model}, Average MRR: {metrics['MRR']:.3f}, Average Top1: {metrics['top1']:.3f}, Feasible: {metrics['feasible']:.3f}, Hallucination: {metrics['hallu']:.3f}, Accuracy: {metrics['acc']:.3f}, len: {metrics['len']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
