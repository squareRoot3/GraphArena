{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('..')\n",
    "os.chdir('..')\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tasks import *\n",
    "import networkx as nx \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'final_results/np_easy'  # 'np_hard' or 'p_easy' or 'p_hard'\n",
    "difficulty = path.split('_')[-1]\n",
    "files = os.listdir(path)\n",
    "merged_responses = {}\n",
    "problem_num = 500\n",
    "dataset_loc = 'dataset'\n",
    "\n",
    "for f in files:\n",
    "    if len(f.split('_')) < 2:\n",
    "        continue\n",
    "    llm, task = f.split('_')[0], f.split('_')[1]\n",
    "    with open(f'{path}/{f}', 'r') as file:\n",
    "        response_dict = json.load(file)\n",
    "    for i in range(0, problem_num):\n",
    "        if task not in merged_responses:\n",
    "            merged_responses[task] = defaultdict(dict)\n",
    "        merged_responses[task][i][llm] = response_dict[str(i)][llm]\n",
    "task_list = list(merged_responses.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['random', 'greedy', 'approximated']:\n",
    "    score = {}\n",
    "    task_name = 'GED'\n",
    "    task= globals()[task_name + '_Task'](dataset_loc)\n",
    "    task.load_dataset(difficulty)\n",
    "    score[task_name] = defaultdict(dict)\n",
    "    for i in range(0, problem_num):\n",
    "        score[task_name][i]['gt'] = task.problem_set[i]['exact_answer']\n",
    "        score[task_name][i]['gt'] = task.approx_solver(task.problem_set[i]['graph'], method=method)\n",
    "        for llm in merged_responses[task_name][i].keys():\n",
    "            if llm == 'problem':\n",
    "                continue\n",
    "            r = merged_responses[task_name][i][llm]\n",
    "            if r is None:\n",
    "                r = ''\n",
    "                print(i, llm, task_name)\n",
    "            score[task_name][i][llm] = task.check_solution(i, r)\n",
    "    metrics = defaultdict(dict)\n",
    "    less_is_better = ['GED', 'TSP', 'MVC', 'Distance']\n",
    "    results = []\n",
    "    task = task_name\n",
    "    model_list = list(score[task][0].keys())\n",
    "    model_list.remove('gt')\n",
    "    for model in model_list:\n",
    "        metrics[task][model] = {'worse': [],'equal':[], 'better':[]}\n",
    "        for i in range(0, problem_num):\n",
    "            diff = score[task][i][model] - score[task][i]['gt']\n",
    "            if task in less_is_better:\n",
    "                diff = -diff\n",
    "            worse = 0 > score[task][i][model] or diff < 0\n",
    "            equal = score[task][i][model] == score[task][i]['gt']\n",
    "            better = 0 <= score[task][i][model] and diff > 0\n",
    "\n",
    "            metrics[task][model]['worse'].append(worse)\n",
    "            metrics[task][model]['equal'].append(equal)\n",
    "            metrics[task][model]['better'].append(better)\n",
    "        avg_worse = sum(metrics[task][model]['worse']) / problem_num\n",
    "        avg_equal = sum(metrics[task][model]['equal']) / problem_num\n",
    "        avg_better = sum(metrics[task][model]['better']) / problem_num\n",
    "        results.append((task, model, avg_worse, avg_equal, avg_better))\n",
    "\n",
    "    sorted_results = defaultdict(list)\n",
    "\n",
    "    task_results = [result for result in results if result[0] == task]\n",
    "    sorted_results[task] = sorted(task_results, key=lambda x: x[2], reverse=False)\n",
    "\n",
    "    # Print sorted results for each task\n",
    "    for task, task_results in sorted_results.items():\n",
    "        print(f\"\\nTask: {task}, method: {method}\")\n",
    "        for result in task_results:\n",
    "            print(f\"Model: {result[1]}, worse: {result[2]:.3f}, equal: {result[3]:.3f}, better: {result[4]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
