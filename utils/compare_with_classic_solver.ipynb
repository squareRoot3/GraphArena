{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append('..')\n",
    "os.chdir('..')\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tasks import *\n",
    "import networkx as nx \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'final_results/np_hard'  # 'np_hard' or 'p_easy' or 'p_hard'\n",
    "difficulty = path.split('_')[-1]\n",
    "files = os.listdir(path)\n",
    "merged_responses = {}\n",
    "problem_num = 500\n",
    "dataset_loc = 'dataset'\n",
    "\n",
    "for f in files:\n",
    "    if len(f.split('_')) < 2:\n",
    "        continue\n",
    "    llm, task = f.split('_')[0], f.split('_')[1]\n",
    "    with open(f'{path}/{f}', 'r') as file:\n",
    "        response_dict = json.load(file)\n",
    "    for i in range(0, problem_num):\n",
    "        if task not in merged_responses:\n",
    "            merged_responses[task] = defaultdict(dict)\n",
    "        merged_responses[task][i][llm] = response_dict[str(i)][llm]\n",
    "task_list = list(merged_responses.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task: TSP, method: chris\n",
      "Model: gpt4, worse: 0.942, equal: 0.004, better: 0.054\n",
      "Model: deepseek, worse: 0.978, equal: 0.000, better: 0.022\n",
      "Model: claude, worse: 0.994, equal: 0.000, better: 0.006\n",
      "Model: llama, worse: 0.994, equal: 0.000, better: 0.006\n",
      "Model: gpt, worse: 0.998, equal: 0.000, better: 0.002\n",
      "Model: qwen7b, worse: 1.000, equal: 0.000, better: 0.000\n",
      "Model: qwen, worse: 1.000, equal: 0.000, better: 0.000\n",
      "Model: llama8b, worse: 1.000, equal: 0.000, better: 0.000\n",
      "Model: mixtral, worse: 1.000, equal: 0.000, better: 0.000\n",
      "Model: gemma, worse: 1.000, equal: 0.000, better: 0.000\n",
      "\n",
      "Task: TSP, method: greedy\n",
      "Model: gpt4, worse: 0.880, equal: 0.028, better: 0.092\n",
      "Model: deepseek, worse: 0.954, equal: 0.002, better: 0.044\n",
      "Model: llama, worse: 0.986, equal: 0.000, better: 0.014\n",
      "Model: claude, worse: 0.990, equal: 0.000, better: 0.010\n",
      "Model: gpt, worse: 0.996, equal: 0.000, better: 0.004\n",
      "Model: llama8b, worse: 0.998, equal: 0.000, better: 0.002\n",
      "Model: qwen7b, worse: 1.000, equal: 0.000, better: 0.000\n",
      "Model: qwen, worse: 1.000, equal: 0.000, better: 0.000\n",
      "Model: mixtral, worse: 1.000, equal: 0.000, better: 0.000\n",
      "Model: gemma, worse: 1.000, equal: 0.000, better: 0.000\n",
      "\n",
      "Task: TSP, method: random\n",
      "Model: gpt4, worse: 0.174, equal: 0.000, better: 0.826\n",
      "Model: llama, worse: 0.204, equal: 0.000, better: 0.796\n",
      "Model: claude, worse: 0.274, equal: 0.000, better: 0.726\n",
      "Model: deepseek, worse: 0.328, equal: 0.000, better: 0.672\n",
      "Model: gpt, worse: 0.332, equal: 0.000, better: 0.668\n",
      "Model: llama8b, worse: 0.702, equal: 0.000, better: 0.298\n",
      "Model: qwen, worse: 0.736, equal: 0.000, better: 0.264\n",
      "Model: mixtral, worse: 0.764, equal: 0.000, better: 0.236\n",
      "Model: qwen7b, worse: 0.770, equal: 0.000, better: 0.230\n",
      "Model: gemma, worse: 0.962, equal: 0.000, better: 0.038\n"
     ]
    }
   ],
   "source": [
    "for method in ['chris', 'greedy', 'random']:\n",
    "    score = {}\n",
    "    task_name = 'TSP'\n",
    "    task= globals()[task_name + '_Task'](dataset_loc)\n",
    "    task.load_dataset(difficulty)\n",
    "    score[task_name] = defaultdict(dict)\n",
    "    for i in range(0, problem_num):\n",
    "        score[task_name][i]['gt'] = task.problem_set[i]['exact_answer']\n",
    "        score[task_name][i]['gt'] = task.approx_solver(task.problem_set[i]['graph'], method=method)[0]\n",
    "        for llm in merged_responses[task_name][i].keys():\n",
    "            if llm == 'problem':\n",
    "                continue\n",
    "            r = merged_responses[task_name][i][llm]\n",
    "            if r is None:\n",
    "                r = ''\n",
    "                print(i, llm, task_name)\n",
    "            score[task_name][i][llm] = task.check_solution(i, r)\n",
    "    metrics = defaultdict(dict)\n",
    "    less_is_better = ['GED', 'TSP', 'MVC', 'Distance']\n",
    "    results = []\n",
    "    task = task_name\n",
    "    model_list = list(score[task][0].keys())\n",
    "    model_list.remove('gt')\n",
    "    for model in model_list:\n",
    "        metrics[task][model] = {'worse': [],'equal':[], 'better':[]}\n",
    "        for i in range(0, problem_num):\n",
    "            worse = 0 > score[task][i][model] or score[task][i][model] > score[task][i]['gt']\n",
    "            equal = score[task][i][model] == score[task][i]['gt']\n",
    "            better = 0 <= score[task][i][model] and score[task][i][model] < score[task][i]['gt']\n",
    "\n",
    "            metrics[task][model]['worse'].append(worse)\n",
    "            metrics[task][model]['equal'].append(equal)\n",
    "            metrics[task][model]['better'].append(better)\n",
    "        avg_worse = sum(metrics[task][model]['worse']) / problem_num\n",
    "        avg_equal = sum(metrics[task][model]['equal']) / problem_num\n",
    "        avg_better = sum(metrics[task][model]['better']) / problem_num\n",
    "        results.append((task, model, avg_worse, avg_equal, avg_better))\n",
    "\n",
    "    sorted_results = defaultdict(list)\n",
    "\n",
    "    task_results = [result for result in results if result[0] == task]\n",
    "    sorted_results[task] = sorted(task_results, key=lambda x: x[2], reverse=False)\n",
    "\n",
    "    # Print sorted results for each task\n",
    "    for task, task_results in sorted_results.items():\n",
    "        print(f\"\\nTask: {task}, method: {method}\")\n",
    "        for result in task_results:\n",
    "            print(f\"Model: {result[1]}, worse: {result[2]:.3f}, equal: {result[3]:.3f}, better: {result[4]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/GED_hard.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     31\u001b[0m     task \u001b[38;5;241m=\u001b[39m GED_Task(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 32\u001b[0m     \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhard\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Determine the number of CPU cores to use\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     num_cores \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mcpu_count()\n",
      "File \u001b[0;32m/mnt/sdc/tjh/GraphArena/tasks/base.py:62\u001b[0m, in \u001b[0;36mNPTask.load_dataset\u001b[0;34m(self, difficulty, example)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_dataset\u001b[39m(\u001b[38;5;28mself\u001b[39m, difficulty\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, example\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m difficulty:\n\u001b[0;32m---> 62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_set \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_loc\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtask_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdifficulty\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproblem_set \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_loc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/GED_hard.pkl'"
     ]
    }
   ],
   "source": [
    "# Run Classic solver\n",
    "import os, sys\n",
    "os.chdir('..')\n",
    "sys.path.append('..')\n",
    "import multiprocessing as mp\n",
    "from tasks import *\n",
    "from openai import OpenAI\n",
    "import networkx as nx\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import fast_tsp\n",
    "import time\n",
    "from functools import partial\n",
    "\n",
    "def process_problem(problem, result_dict):\n",
    "    exact_answer, path = task.exact_solver(*problem['graph'])\n",
    "    print(f\"Processed problem {problem['id']}\")\n",
    "    result_dict[problem['id']] = {'exact_answer': exact_answer, 'path': path}\n",
    "\n",
    "def save_results(task, result_dict):\n",
    "    for problem_id, result in result_dict.items():\n",
    "        idx = next(i for i, prob in enumerate(task.problem_set) if prob['id'] == problem_id)\n",
    "        task.problem_set[idx]['exact_answer'] = result['exact_answer']\n",
    "        task.problem_set[idx]['path'] = result['path']\n",
    "    task.save_dataset('hard')\n",
    "    print(\"Results saved.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    task = GED_Task('dataset')\n",
    "    task.load_dataset('hard')\n",
    "\n",
    "    # Determine the number of CPU cores to use\n",
    "    num_cores = mp.cpu_count()\n",
    "\n",
    "    # Create a manager for shared objects\n",
    "    manager = mp.Manager()\n",
    "    result_dict = manager.dict()\n",
    "\n",
    "    # Create a pool of worker processes\n",
    "    pool = mp.Pool(processes=num_cores)\n",
    "\n",
    "    # Start the processing\n",
    "    for problem in task.problem_set[:500]:\n",
    "        pool.apply_async(process_problem, args=(problem, result_dict))\n",
    "\n",
    "    # Monitor and save results every 10 minutes\n",
    "    start_time = time.time()\n",
    "    while pool._cache:\n",
    "        time.sleep(10)  # Check every 10 seconds\n",
    "        if time.time() - start_time >= 600:  # 600 seconds = 10 minutes\n",
    "            save_results(task, result_dict)\n",
    "            start_time = time.time()\n",
    "\n",
    "    # Close the pool and wait for all processes to finish\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Final save\n",
    "    save_results(task, result_dict)\n",
    "\n",
    "    print(\"Processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
