{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from tasks import *\n",
    "import numpy as np\n",
    "\n",
    "path = 'final_results/np_easy'  # 'np_hard' or 'p_easy' or 'p_hard' \n",
    "difficulty = 'easy'\n",
    "files = os.listdir(path)\n",
    "merged_responses = {}\n",
    "problem_num = 500\n",
    "dataset_loc = 'dataset'\n",
    "\n",
    "for f in files:\n",
    "    if len(f.split('_')) < 2:\n",
    "        continue\n",
    "    llm, task = f.split('_')[0], f.split('_')[1]\n",
    "    with open(f'{path}/{f}', 'r') as file:\n",
    "        response_dict = json.load(file)\n",
    "    for i in range(0, problem_num):\n",
    "        if task not in merged_responses:\n",
    "            merged_responses[task] = defaultdict(dict)\n",
    "        merged_responses[task][i][llm] = response_dict[str(i)][llm]\n",
    "task_list = list(merged_responses.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scoring process may take a few minutes.\n",
    "# If want to evaluate on specific tasks and models:\n",
    "# task_list = ['GED', 'MCP', 'MCS', 'TSP']\n",
    "# model_list = ['deepseek', 'llama', 'llama8b', 'gpt', 'gpt4', 'claude', 'mixtral']\n",
    "score = {}\n",
    "for task_name in task_list:\n",
    "    task= globals()[task_name + '_Task'](dataset_loc)\n",
    "    task.load_dataset(difficulty)\n",
    "    score[task_name] = defaultdict(dict)\n",
    "    for i in range(0, problem_num):\n",
    "        score[task_name][i]['gt'] = task.problem_set[i]['exact_answer']\n",
    "        if task_name in ['GED', 'TSP', 'MCS'] and difficulty=='hard':\n",
    "            score[task_name][i]['gt'] = task.problem_set[i]['approx_answer']\n",
    "        for llm in merged_responses[task_name][i].keys():\n",
    "            if llm == 'problem':\n",
    "                continue\n",
    "            r = merged_responses[task_name][i][llm]\n",
    "            if r is None:\n",
    "                r = ''\n",
    "                print(i, llm, task_name)\n",
    "            score[task_name][i][llm] = task.check_solution(i, r)\n",
    "# json.dump(score, open('score.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = defaultdict(dict)\n",
    "less_is_better = ['GED', 'TSP', 'MVC', 'Distance']\n",
    "results = []\n",
    "for task in task_list:\n",
    "    print(task)\n",
    "    model_list = list(score[task][0].keys())\n",
    "    model_list.remove('gt')\n",
    "    for model in model_list:\n",
    "        metrics[task][model] = {'rank':[], 'feasible':[], 'MRR':[], 'hallu':[], 'acc': [],'top1':[], 'top3':[], 'len': []}\n",
    "        for i in range(0, problem_num):\n",
    "            metrics[task][model]['feasible'].append(score[task][i][model]>0)\n",
    "            metrics[task][model]['hallu'].append(score[task][i][model]==-2)\n",
    "            metrics[task][model]['len'].append(len(merged_responses[task_name][i][model]))\n",
    "            if task in ['GED', 'TSP', 'MVC']:\n",
    "                acc = 0 <= score[task][i][model] and score[task][i][model] <= score[task][i]['gt']\n",
    "            elif task in ['MCP', 'MCS', 'MIC']:\n",
    "                acc = score[task][i][model] >= score[task][i]['gt']\n",
    "            else:\n",
    "                acc = score[task][i][model] == score[task][i]['gt']\n",
    "            \n",
    "            metrics[task][model]['acc'].append(acc)\n",
    "            \n",
    "            rank = 1\n",
    "            error_knt = 0\n",
    "            for model2 in model_list:\n",
    "                if score[task][i][model2] < 0:\n",
    "                    error_knt += 1\n",
    "                if task in less_is_better:\n",
    "                    if score[task][i][model] > score[task][i][model2] and score[task][i][model2] >= 0:\n",
    "                        rank += 1\n",
    "                else:\n",
    "                    if score[task][i][model] < score[task][i][model2] and score[task][i][model2] >= 0:\n",
    "                        rank += 1\n",
    "            if score[task][i][model] < 0:\n",
    "                rank = len(model_list)\n",
    "            if error_knt == len(model_list):\n",
    "                continue\n",
    "            metrics[task][model]['rank'].append(rank)\n",
    "            metrics[task][model]['top1'].append(rank==1)\n",
    "            metrics[task][model]['top3'].append(rank<=3)\n",
    "            metrics[task][model]['MRR'].append(1/rank)\n",
    "        avg_rank = np.mean(metrics[task][model]['rank'])\n",
    "        avg_feasible = sum(metrics[task][model]['feasible']) / problem_num\n",
    "        avg_MRR = np.mean(metrics[task][model]['MRR'])\n",
    "        avg_hallu = sum(metrics[task][model]['hallu']) / problem_num\n",
    "        avg_acc = sum(metrics[task][model]['acc']) / problem_num\n",
    "        avg_top1 = np.mean(metrics[task][model]['top1'])\n",
    "        avg_top3 = np.mean(metrics[task][model]['top3'])\n",
    "        avg_len = np.mean(metrics[task][model]['len'])\n",
    "        results.append((task, model, avg_top1, avg_top3, avg_MRR, avg_feasible, avg_hallu, avg_acc, avg_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the results by MRR for each task\n",
    "sorted_results = defaultdict(list)\n",
    "\n",
    "for task in task_list:\n",
    "    task_results = [result for result in results if result[0] == task]\n",
    "    sorted_results[task] = sorted(task_results, key=lambda x: x[5], reverse=True)  # Sort by MRR\n",
    "\n",
    "# Print sorted results for each task\n",
    "for task, task_results in sorted_results.items():\n",
    "    print(f\"\\nTask: {task}\")\n",
    "    for result in task_results:\n",
    "        print(f\"Model: {result[1]}, Top1: {result[2]:.3f}, Top3: {result[3]:.3f}, MRR: {result[4]:.3f}, Feasible: {result[5]:.3f}, Hallucination: {result[6]:.3f}, Accuracy: {result[7]:.3f}, len: {result[8]:.3f}\")\n",
    "\n",
    "# Calculate average MRR performance across all tasks for each model\n",
    "model_metrics = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# Aggregate metrics for each model across tasks\n",
    "for result in results:\n",
    "    task, model, avg_top1, avg_top3, avg_MRR, avg_feasible, avg_hallu, avg_acc, avg_len = result\n",
    "    model_metrics[model]['MRR'].append(avg_MRR)\n",
    "    model_metrics[model]['top1'].append(avg_top1)\n",
    "    model_metrics[model]['top3'].append(avg_top3)\n",
    "    model_metrics[model]['feasible'].append(avg_feasible)\n",
    "    model_metrics[model]['hallu'].append(avg_hallu)\n",
    "    model_metrics[model]['acc'].append(avg_acc)\n",
    "    model_metrics[model]['len'].append(avg_len)\n",
    "    \n",
    "# Compute average metrics for each model and sort models by their average MRR\n",
    "average_metrics_performance = {model: {metric: sum(values) / len(values) for metric, values in metrics.items()} for model, metrics in model_metrics.items()}\n",
    "sorted_average_metrics = sorted(average_metrics_performance.items(), key=lambda x: x[1]['MRR'], reverse=True)\n",
    "\n",
    "# Print the sorted average metrics for each model\n",
    "print(\"\\nAverage Performance Across All Tasks:\")\n",
    "for model, metrics in sorted_average_metrics:\n",
    "    print(f\"Model: {model}, Average MRR: {metrics['MRR']:.3f}, Average Top1: {metrics['top1']:.3f}, Feasible: {metrics['feasible']:.3f}, Hallucination: {metrics['hallu']:.3f}, Accuracy: {metrics['acc']:.3f}, len: {metrics['len']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
